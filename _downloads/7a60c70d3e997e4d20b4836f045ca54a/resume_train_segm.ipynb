{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nExample 09: Resume training a network for segmentation\n======================================================\n\nThis script resumes an earlier training of a MS-D network for \nsegmentation (i.e. labeling)\nRun generatedata.py first to generate required training data, and\ntrain_segm.py to generate a partially trained network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Import code\nimport msdnet\nimport glob\n\n# Define training data\n# First, create lists of input files (noisy) and target files (labels)\nflsin = sorted(glob.glob('train/noisy/*.tiff'))\nflstg = sorted(glob.glob('train/label/*.tiff'))\n# Create list of datapoints (i.e. input/target pairs)\ndats = []\nfor i in range(len(flsin)):\n    # Create datapoint with file names\n    d = msdnet.data.ImageFileDataPoint(flsin[i],flstg[i])\n    # Convert datapoint to one-hot, using labels 0, 1, 2, 3, and 4,\n    # which are the labels given in each label TIFF file.\n    d_oh = msdnet.data.OneHotDataPoint(d, [0,1,2,3,4])\n    # Augment data by rotating and flipping\n    d_augm = msdnet.data.RotateAndFlipDataPoint(d_oh)\n    # Add augmented datapoint to list\n    dats.append(d_augm)\n# Note: The above can also be achieved using a utility function for such 'simple' cases:\n# dats = msdnet.utils.load_simple_data('train/noisy/*.tiff', 'train/label/*.tiff', augment=True, labels=[0,1,2,3,4])\n\n# Use image batches of a single image\nbprov = msdnet.data.BatchProvider(dats,1)\n\n# Define validation data (not using augmentation)\nflsin = sorted(glob.glob('val/noisy/*.tiff'))\nflstg = sorted(glob.glob('val/label/*.tiff'))\ndatsv = []\nfor i in range(len(flsin)):\n    d = msdnet.data.ImageFileDataPoint(flsin[i],flstg[i])\n    d_oh = msdnet.data.OneHotDataPoint(d, [0,1,2,3,4])\n    datsv.append(d_oh)\n# Note: The above can also be achieved using a utility function for such 'simple' cases:\n# datsv = msdnet.utils.load_simple_data('train/noisy/*.tiff', 'train/label/*.tiff', augment=False, labels=[0,1,2,3,4])\n\n# Load network, training algorithm, and validation object from checkpoint of previous training\nn, t, val = msdnet.train.restore_training('segm_params.checkpoint', msdnet.network.SegmentationMSDNet, msdnet.train.AdamAlgorithm, msdnet.validate.MSEValidation, datsv, gpu=True)\n\n# Log error metrics to console\nconsolelog = msdnet.loggers.ConsoleLogger()\n# Log error metrics to file\nfilelog = msdnet.loggers.FileLogger('log_segm.txt')\n# Log typical, worst, and best images to image files\nimagelog = msdnet.loggers.ImageLabelLogger('log_segm', onlyifbetter=True)\n# Log typical, worst, and best images to image files\n# Output probability map for a single channel (in this case, channel 3)\nsinglechannellog = msdnet.loggers.ImageLogger('log_segm_singlechannel', chan_out=3, onlyifbetter=True)\n\n# Train network until program is stopped manually\n# Network parameters are saved in segm_params.h5\n# Validation is run after every len(datsv) (=25)\n# training steps.\nmsdnet.train.train(n, t, val, bprov, 'segm_params_resumed.h5',loggers=[consolelog,filelog,imagelog,singlechannellog], val_every=len(datsv))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}