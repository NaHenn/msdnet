{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nExample 01: Train a network for regression\n==========================================\n\nThis script trains a MS-D network for regression (i.e. denoising/artifact removal)\nRun generatedata.py first to generate required training data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Import code\nimport msdnet\nimport glob\n\n# Define dilations in [1,10] as in paper.\ndilations = msdnet.dilations.IncrementDilations(10)\n\n# Create main network object for regression, with 100 layers,\n# [1,10] dilations, 1 input channel, 1 output channel, using\n# the GPU (set gpu=False to use CPU)\nn = msdnet.network.MSDNet(100, dilations, 1, 1, gpu=True)\n\n# Initialize network parameters\nn.initialize()\n\n# Define training data\n# First, create lists of input files (noisy) and target files (noiseless)\nflsin = sorted(glob.glob('train/noisy/*.tiff'))\nflstg = sorted(glob.glob('train/noiseless/*.tiff'))\n# Create list of datapoints (i.e. input/target pairs)\ndats = []\nfor i in range(len(flsin)):\n    # Create datapoint with file names\n    d = msdnet.data.ImageFileDataPoint(flsin[i],flstg[i])\n    # Augment data by rotating and flipping\n    d_augm = msdnet.data.RotateAndFlipDataPoint(d)\n    # Add augmented datapoint to list\n    dats.append(d_augm)\n# Note: The above can also be achieved using a utility function for such 'simple' cases:\n# dats = msdnet.utils.load_simple_data('train/noisy/*.tiff', 'train/noiseless/*.tiff', augment=True)\n\n# Normalize input and output of network to zero mean and unit variance using\n# training data images\nn.normalizeinout(dats)\n\n# Use image batches of a single image\nbprov = msdnet.data.BatchProvider(dats,1)\n\n# Define validation data (not using augmentation)\nflsin = sorted(glob.glob('val/noisy/*.tiff'))\nflstg = sorted(glob.glob('val/noiseless/*.tiff'))\ndatsv = []\nfor i in range(len(flsin)):\n    d = msdnet.data.ImageFileDataPoint(flsin[i],flstg[i])\n    datsv.append(d)\n# Note: The above can also be achieved using a utility function for such 'simple' cases:\n# datsv = msdnet.utils.load_simple_data('val/noisy/*.tiff', 'val/noiseless/*.tiff', augment=False)\n\n# Validate with Mean-Squared Error\nval = msdnet.validate.MSEValidation(datsv)\n\n# Use ADAM training algorithms\nt = msdnet.train.AdamAlgorithm(n)\n\n# Log error metrics to console\nconsolelog = msdnet.loggers.ConsoleLogger()\n# Log error metrics to file\nfilelog = msdnet.loggers.FileLogger('log_regr.txt')\n# Log typical, worst, and best images to image files\nimagelog = msdnet.loggers.ImageLogger('log_regr', onlyifbetter=True)\n\n# Train network until program is stopped manually\n# Network parameters are saved in regr_params.h5\n# Validation is run after every len(datsv) (=25)\n# training steps.\nmsdnet.train.train(n, t, val, bprov, 'regr_params.h5',loggers=[consolelog,filelog,imagelog], val_every=len(datsv))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}