{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nExample 07: Train a network for segmentation (tomography)\n=========================================================\n\nThis script trains a MS-D network for segmentation (i.e. labeling)\nRun generatedata.py first to generate required training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Import code\nimport msdnet\nimport glob\n\n# Define dilations in [1,10] as in paper.\ndilations = msdnet.dilations.IncrementDilations(10)\n\n# Create main network object for segmentation, with 100 layers,\n# [1,10] dilations, 5 input channels (5 slices), 4 output channels (one for each label), \n# using the GPU (set gpu=False to use CPU)\nn = msdnet.network.SegmentationMSDNet(100, dilations, 5, 4, gpu=True)\n\n# Initialize network parameters\nn.initialize()\n\n# Define training data\n# First, create lists of input files (low quality) and target files (labels)\nflsin = sorted(glob.glob('tomo_train/lowqual/*.tiff'))\nflstg = sorted(glob.glob('tomo_train/label/*.tiff'))\n# Create list of datapoints (i.e. input/target pairs)\ndats = []\nfor i in range(len(flsin)):\n    # Create datapoint with file names\n    d = msdnet.data.ImageFileDataPoint(flsin[i],flstg[i])\n    # Convert datapoint to one-hot, using labels 0, 1, 2, and 3,\n    # which are the labels given in each label TIFF file.\n    d_oh = msdnet.data.OneHotDataPoint(d, [0,1,2,3])\n    # Add datapoint to list\n    dats.append(d_oh)\n# Note: The above can also be achieved using a utility function for such 'simple' cases:\n# dats = msdnet.utils.load_simple_data('tomo_train/lowqual/*.tiff', 'tomo_train/label/*.tiff', augment=False, labels=[0,1,2,3])\n\n# Convert input slices to input slabs (i.e. multiple slices as input)\ndats = msdnet.data.convert_to_slabs(dats, 2, flip=True)\n# Augment data by rotating and flipping\ndats_augm = [msdnet.data.RotateAndFlipDataPoint(d) for d in dats]\n    \n# Normalize input and output of network to zero mean and unit variance using\n# training data images\nn.normalizeinout(dats)\n\n# Use image batches of a single image\nbprov = msdnet.data.BatchProvider(dats,1)\n\n# Define validation data (not using augmentation)\nflsin = sorted(glob.glob('tomo_val/lowqual/*.tiff'))\nflstg = sorted(glob.glob('tomo_val/label/*.tiff'))\ndatsv = []\nfor i in range(len(flsin)):\n    d = msdnet.data.ImageFileDataPoint(flsin[i],flstg[i])\n    d_oh = msdnet.data.OneHotDataPoint(d, [0,1,2,3])\n    datsv.append(d_oh)\n# Note: The above can also be achieved using a utility function for such 'simple' cases:\n# datsv = msdnet.utils.load_simple_data('tomo_val/lowqual/*.tiff', 'tomo_val/label/*.tiff', augment=False, labels=[0,1,2,3])\n\n# Convert input slices to input slabs (i.e. multiple slices as input)\ndatsv = msdnet.data.convert_to_slabs(datsv, 2, flip=False)\n\n# Validate with Mean-Squared Error\nval = msdnet.validate.MSEValidation(datsv)\n\n# Use ADAM training algorithms\nt = msdnet.train.AdamAlgorithm(n)\n\n# Log error metrics to console\nconsolelog = msdnet.loggers.ConsoleLogger()\n# Log error metrics to file\nfilelog = msdnet.loggers.FileLogger('log_tomo_segm.txt')\n# Log typical, worst, and best images to image files\nimagelog = msdnet.loggers.ImageLabelLogger('log_tomo_segm', chan_in=2, onlyifbetter=True)\n# Log typical, worst, and best images to image files\n# Output probability map for a single channel (in this case, channel 3)\nsinglechannellog = msdnet.loggers.ImageLogger('log_tomo_segm_singlechannel', chan_in=2, chan_out=3, onlyifbetter=True)\n\n# Train network until program is stopped manually\n# Network parameters are saved in segm_params.h5\n# Validation is run after every len(datsv) (=256)\n# training steps.\nmsdnet.train.train(n, t, val, bprov, 'tomo_segm_params.h5',loggers=[consolelog,filelog,imagelog,singlechannellog], val_every=len(datsv))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}